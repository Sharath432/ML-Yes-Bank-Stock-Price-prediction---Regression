{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - INDIVIDUAL\n",
        "##### **Team Member 1 -**    SARATHRAJ R\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project aims to analyze and predict the monthly closing prices of Yes Bank, a significant player in the Indian financial sector. The dataset spans from the bank's inception to the present, encompassing monthly stock prices, including opening, closing, highest, and lowest values for each month. The primary goal is to predict the monthly closing price of Yes Bank's stock using various machine learning and time series models.\n",
        "\n",
        "The project begins with comprehensive data exploration and preprocessing using Pandas for efficient data manipulation and aggregation. This involves cleaning the data, handling missing values, and ensuring it is in a suitable format for analysis. Visualizations will be created using Matplotlib and Seaborn to understand the trends, seasonal patterns, and potential anomalies in the stock prices, particularly around significant events like the fraud case revelation. These visualizations help in gaining insights into the stock’s behavior and its correlation with various factors over time.\n",
        "\n",
        "For the computational aspect, NumPy will be utilized for performing efficient numerical operations on the dataset. This includes operations like normalization, transformation, and other preprocessing steps that are crucial for preparing the data for modeling. The core of the project involves leveraging Scikit Learn for model training, optimization, and evaluation. Various time series models, such as ARIMA, SARIMA, and Prophet, along with machine learning models like Linear Regression, Random Forest, and Gradient Boosting, will be employed to predict the stock's closing price. Model performance will be evaluated using metrics like Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and R-squared (R²) to determine the best-performing model.\n",
        "\n",
        "The project architecture involves several key stages. Initially, data collection and preprocessing form the foundation, ensuring the dataset is clean and structured. Next, exploratory data analysis (EDA) provides valuable insights into the stock price trends and their driving factors. Following this, feature engineering and selection help in identifying the most relevant features that influence the stock’s closing price. The modeling phase includes training various models and fine-tuning their parameters to enhance predictive accuracy. Finally, model evaluation and validation are conducted to assess the models' performance and select the best one for predicting the future closing prices.\n",
        "\n",
        "Overall, this project integrates data analysis, visualization, and predictive modeling to understand and forecast the monthly closing prices of Yes Bank's stock."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting Monthly Closing Stock Prices for Yes Bank .\n",
        "\n",
        "The objective of this project is to predict the monthly closing stock price of Yes Bank. Given the historical stock price data since its inception, including the opening, closing, highest, and lowest prices for each month, the goal is to develop a predictive model that can accurately forecast the closing price."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import norm\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
        "import itertools"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "feNW5d3jQAs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "\n",
        "pathway = '/content/drive/MyDrive/data_YesBank_StockPrices.csv'\n",
        "df = pd.read_csv(pathway)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "OK8GSk2CQ4zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df.isnull())\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset comprises monthly stock prices of Yes Bank, including the closing, starting, highest, and lowest stock prices from its inception to the present. The primary objective is to predict the stock's closing price for each month.\n",
        "\n",
        "Eventually, I dont find any missing or duplicates in the data set upon examining with heat map.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns\n"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Date - month of the candles\n",
        "\n",
        "Open - opening price of the month\n",
        "\n",
        "Hight - highest price of the month\n",
        "\n",
        "Low - lowest price of the month\n",
        "\n",
        "close- closing price of the month"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df.columns:\n",
        "  print(\"No. of unique values in \",i,\"is\",df[i].nunique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "OnpZYU2c3Wub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Data Inspection\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "print(df.describe())"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Cleaning\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "ujSSscFoSWq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# visualize Outliers\n",
        "plt.figure(figsize=(12, 6))\n",
        "df[['Open', 'High', 'Low', 'Close']].boxplot()\n",
        "plt.xlabel('Price Type')\n",
        "plt.ylabel('Price')\n",
        "plt.title('Box Plot of Stock Prices')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fmZ-W8BKSWnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Set the Date column as index\n",
        "\n",
        "df.set_index('Date',inplace = True)"
      ],
      "metadata": {
        "id": "7DtJBkShSuZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'Date' column has been moved to the index position, and it is no longer a column in the DataFrame.\n",
        "\n",
        "Upon examining we could see no missing values and no duplicates and we can also see that there are some outliers in our features lets treat this accordingly in future steps.\n",
        "\n",
        "And describing the dataset we have examined stock prices stat values of mean, median and aggregations."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "\n",
        "\n",
        "# Chart - 1 Box Plot\n",
        "sns.boxplot(df['Close'])\n",
        "plt.title('Boxplot of Close')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A box plot was chosen to visualize the distribution of the closing price and identify potential outliers.\n",
        "\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The box plot shows the distribution of Yes Bank's closing prices. We can see the median closing price, the quartiles, and potential outliers, which are the data points beyond the whiskers of the box plot."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifying outliers can help in refining the predictive model, leading to more accurate predictions of Yes Bank's closing prices. This can be valuable for making informed investment decisions. However, further analysis is needed to determine if these outliers represent actual market trends or data anomalies."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "sns.boxplot(df['Open'])\n",
        "plt.title('Boxplot of Open')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the closing price, a box plot was used to visualize the distribution of the opening price and identify potential outliers.\n",
        "\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The box plot for 'Open' shows the distribution of Yes Bank's opening prices. It helps visualize the median opening price, quartiles, and potential outliers (data points beyond the whiskers). This gives an idea of the typical range of opening prices and any extreme values.\n",
        "\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding the distribution of opening prices, including outliers, can help traders and investors make more informed decisions. Identifying patterns and anomalies in opening prices can be useful for developing trading strategies or risk management."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "sns.lineplot(df['High'])\n",
        "plt.xlim(0,100)\n",
        "plt.title('Lineplot of High')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line chart was chosen to visualize the trend of Yes Bank's highest stock price over time.\n",
        "\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line chart shows the fluctuations in the highest price of Yes Bank's stock over time. It reveals periods of growth, decline, and potential volatility.\n",
        "\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, understanding the historical trend of the highest stock price can be useful for investors and traders to identify potential buying or selling opportunities."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Chart - 4 Area Chart\n",
        "df.plot.area(y='High')\n",
        "plt.title('Area Chart of High')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An area chart was chosen to visualize the cumulative trend of the highest stock price over time, emphasizing the magnitude of change."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The area chart shows the cumulative growth and decline of Yes Bank's highest stock price. It highlights periods of significant price changes and the overall trend.\n",
        "\n"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, visualizing the cumulative trend can help investors understand the overall performance of the stock and identify periods of substantial growth or decline."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Chart - 5 histogram\n",
        "sns.histplot(df['Low'])\n",
        "plt.title('Histogram of Low')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram was chosen to visualize the distribution of Yes Bank's lowest stock prices, showing the frequency of different price ranges.\n",
        "\n"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram reveals the distribution of the lowest stock prices, indicating the most common price ranges and potential outliers. It helps understand the frequency of low prices.\n",
        "\n"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, understanding the distribution of low prices can be useful for investors to assess the risk associated with the stock and make informed decisions.\n",
        "\n"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 Moving average chart\n",
        "df['MA10'] = df['Close'].rolling(window=10).mean()\n",
        "df['MA20'] = df['Close'].rolling(window=20).mean()\n",
        "df.plot(y=['Close', 'MA10', 'MA20'])\n",
        "plt.title('Candlestick Chart of Close')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A moving average chart was chosen to visualize the trend of Yes Bank's closing price while smoothing out short-term fluctuations, providing a clearer picture of the overall trend.\n",
        "\n"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows the closing price along with its 10-day and 20-day moving averages. It helps identify trends and potential buy/sell signals based on the crossover of the moving averages.\n",
        "\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, using moving averages can help traders and investors identify potential trend reversals and make more informed trading decisions.\n",
        "\n"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 Horizontal bar chart\n",
        "df.plot.barh(y='Close')\n",
        "plt.title('Horizontal Bar Chart of Close')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A horizontal bar chart was chosen to visualize the closing prices over time in a horizontal format, allowing for easier comparison of prices across different dates.\n",
        "\n"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart displays the closing prices for each date horizontally, making it easy to compare the closing prices across different periods.\n",
        "\n"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the horizontal bar chart can help identify periods of high and low closing prices, which can be useful for traders and investors to understand historical price patterns.\n",
        "\n"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 Scatter plot\n",
        "sns.scatterplot(x='High', y='Low', data=df)\n",
        "plt.title('Scatter Plot of High and Low')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A scatter plot was chosen to visualize the relationship between Yes Bank's highest and lowest stock prices, exploring potential correlations or patterns.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatter plot shows the relationship between the highest and lowest prices for each period. It helps identify any correlation or trends between these two variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, understanding the relationship between high and low prices can be useful for traders to assess the volatility and potential trading range of the stock."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Chart - 9 Linr Chart For \"Low\"\n",
        "sns.lineplot(df['Low'])\n",
        "plt.title('Lineplot of Low')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line chart was chosen to visualize the trend of Yes Bank's lowest stock price over time.\n",
        "\n"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line chart shows the fluctuations in the lowest price of Yes Bank's stock over time. It reveals periods of growth, decline, and potential volatility in the lower price range.\n",
        "\n"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, understanding the historical trend of the lowest stock price can be useful for investors and traders to identify potential buying opportunities or assess downside risk."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Chart - 10 visualization code\n",
        "sns.regplot(data=df, x='Close', y='Open')\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line charts are excellent for showing trends over time, such as changes in sales, temperature, stock prices, etc. They make it easy to identify upward, downward, or cyclical trends."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this chart, we compare the stock's opening and closing prices.\n",
        "\n"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open and Close prices are generally close because they’re on the same trading day/month.\n",
        "\n",
        "If points lie close to a 45-degree line, it suggests little daily/monthly change — markets opened and closed around similar values.\n",
        "\n",
        "If there’s significant scatter, it may indicate volatile trading within the month."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mplfinance\n",
        "\n",
        "import mplfinance as mpf\n",
        "\n",
        "# Ensure date is datetime and set as index\n",
        "df_candle = df.copy()\n",
        "df_candle = df_candle.reset_index()\n",
        "df_candle['Date'] = pd.to_datetime(df_candle['Date'], format='%b-%y')\n",
        "df_candle.set_index('Date', inplace=True) # Set 'Date' back as index\n",
        "\n",
        "# Plot candlestick\n",
        "mpf.plot(df_candle, type='candle', style='charles', title=\"Monthly Candlestick Chart\", volume=False)"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the candlestick chart because it is a standard and powerful visual tool in financial markets. It shows all four key prices — Open, High, Low, Close (OHLC) — in a single compact visual per month, making it ideal for detecting monthly sentiment and volatility."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certain months showed long candles and wicks, indicating extreme volatility — consistent with Yes Bank's crisis period.\n",
        "\n",
        "There are periods where the closing price is consistently lower than the opening price, suggesting bearish sentiment and lack of investor confidence.\n",
        "\n",
        "Volatility spikes during and after significant events like the Rana Kapoor fraud case in 2018, reflecting market reaction."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. These insights can:\n",
        "\n",
        "Help investors time entries/exits better by recognizing volatile periods.\n",
        "\n",
        "Allow risk managers to build volatility buffers during months of historic turbulence.\n",
        "\n",
        "Assist analysts in mapping event-driven price behavior, which strengthens predictive modeling.\n",
        "\n"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_rolling = df_candle.copy()\n",
        "df_rolling['Close_rolling_mean'] = df_rolling['Close'].rolling(window=6).mean()\n",
        "df_rolling['Close_rolling_std'] = df_rolling['Close'].rolling(window=6).std()\n",
        "\n",
        "df_rolling[['Close', 'Close_rolling_mean', 'Close_rolling_std']].plot(figsize=(10, 5), title=\"Rolling Mean & Volatility\")\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart was selected to analyze both trends and volatility over time using rolling statistics (mean and standard deviation). It helps identify smoothed trends and price stability over a 6-month window."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Long-term declining trend visible between 2018–2020, followed by price stabilization.\n",
        "\n",
        "Significant volatility spikes occur in crisis periods — volatility increased sharply post-2018 and again during COVID.\n",
        "\n",
        "The price becomes range-bound in recent years, indicating reduced market confidence."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. These patterns help:\n",
        "\n",
        "Risk teams forecast future volatility and adjust portfolios accordingly.\n",
        "\n",
        "Model developers create features that capture volatility\n",
        "\n",
        "Stakeholders interpret market sentiment changes, allowing for proactive risk assessment and intervention."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "for col in ['Open', 'High', 'Low', 'Close']:\n",
        "    df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', '').str.strip(), errors='coerce')\n",
        "\n",
        "# Drop rows with NaNs\n",
        "df_cleaned = df.dropna(subset=['Open', 'High', 'Low', 'Close'])\n",
        "\n",
        "# Reshape the data for violin plot\n",
        "df_melted = df_cleaned[['Open', 'High', 'Low', 'Close']].melt(var_name='Price_Type', value_name='Value')\n",
        "\n",
        "# Create the violin plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.violinplot(data=df_melted, x='Price_Type', y='Value')\n",
        "plt.title(\"Violin Plot of Stock Price Types (Open, High, Low, Close)\")\n",
        "plt.ylabel(\"Stock Price\")\n",
        "plt.xlabel(\"Price Type\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The violin plot combines the benefits of a boxplot and a KDE (kernel density estimate).\n",
        "\n",
        "It shows:\n",
        "\n",
        "Distribution shape\n",
        "\n",
        "Central tendency (median)\n",
        "\n",
        "Spread (interquartile range)\n",
        "\n",
        "Presence of outliers and fat tails\n",
        "\n",
        "This makes it a powerful tool to visualize stock price volatility and distribution asymmetry."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The High and Low prices have wider distributions, reflecting greater variability and intramonth volatility.\n",
        "\n",
        "The Close prices are slightly right-skewed, indicating rare spikes in value.\n",
        "\n",
        "The Open vs Close prices have similar median ranges, but Close tends to show more spread, suggesting intramonth movement."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Understanding the distribution and volatility of each price type helps:\n",
        "\n",
        "Modelers to engineer better target and feature transformations\n",
        "\n",
        "Traders to assess the risk/reward trade-off for different price levels\n",
        "\n",
        "Stakeholders to interpret market stability and investor sentiment"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add month and year columns\n",
        "\n",
        "df_heatmap = df_candle.copy()\n",
        "df_heatmap['Year'] = df_heatmap.index.year\n",
        "df_heatmap['Month'] = df_heatmap.index.month_name().str[:3]\n",
        "\n",
        "# Pivot table for heatmap\n",
        "pivot_table = df_heatmap.pivot_table(values='Close', index='Month', columns='Year')\n",
        "\n",
        "# Reorder months\n",
        "month_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
        "               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "pivot_table = pivot_table.reindex(month_order)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(pivot_table, annot=True, fmt=\".1f\", cmap=\"YlGnBu\")\n",
        "plt.title(\"Monthly Avg Closing Price Heatmap by Year\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the heatmap to explore seasonal trends in the stock's performance by comparing average monthly closing prices across different years. It visually highlights recurring strong/weak months and detects patterns or anomalies."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some months like January and March tend to have lower average closing prices, possibly due to fiscal closing behaviors or budget announcements.\n",
        "\n",
        "Pre-2018 years had higher seasonal variability, while post-crisis years appear relatively flat — signaling investor fatigue or institutional hold.\n",
        "\n",
        "Helps identify months that may perform better, based on historical averages."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Will the gained insights help create a positive business impact?"
      ],
      "metadata": {
        "id": "q1Vhtvvqf145"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, absolutely:\n",
        "\n",
        "Traders can build seasonality-aware strategies.\n",
        "\n",
        "Portfolio managers can optimize entry/exit windows based on favorable historical patterns.\n",
        "\n",
        "ML models can use month-based features to enhance prediction accuracy, especially if seasonality exists."
      ],
      "metadata": {
        "id": "X4fVOyycf5xg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "\n",
        "for col in ['Open', 'High', 'Low', 'Close']:\n",
        "    df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', '').str.strip(), errors='coerce')\n",
        "\n",
        "# Drop rows with NaNs after conversion\n",
        "df_cleaned = df.dropna(subset=['Open', 'High', 'Low', 'Close'])\n",
        "\n",
        "\n",
        "sns.pairplot(df_cleaned[['Open', 'High', 'Low', 'Close']], diag_kind='kde')\n",
        "plt.suptitle(\"Pair Plot of OHLC Stock Prices\", y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the pair plot because it provides a comprehensive view of how each of the four main stock price metrics — Open, High, Low, and Close — are related to each other. Unlike single plots, a pair plot displays:\n",
        "\n",
        "Scatter plots for every pair of variables (bivariate relationships)\n",
        "\n",
        "Distribution plots for each variable on the diagonal\n",
        "\n",
        "Optional kernel density estimates to observe distribution shape\n",
        "\n",
        "This makes it ideal for:\n",
        "\n",
        "Identifying multicollinearity\n",
        "\n",
        "Spotting linear/nonlinear relationships\n",
        "\n",
        "Understanding data distributions and spread\n",
        "\n"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Strong linear correlations exist among all four features — especially between:\n",
        "\n",
        "High vs Close, Low vs Open, and Open vs Close\n",
        "\n",
        "This is expected due to the nature of monthly stock data\n",
        "\n",
        "The scatter plots show that all price variables tend to move together, suggesting that one can potentially be used to predict another.\n",
        "\n",
        "Distributions are right-skewed, especially for High and Close, indicating that the stock has had occasional sharp price spikes, but spent most months at lower price levels.\n",
        "\n"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Will the gained insights help create a positive business impact?"
      ],
      "metadata": {
        "id": "5n9Dgws5gst0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes — the insights from the pair plot are valuable both for modeling and strategic analysis:\n",
        "\n",
        "Business & Modeling Impact:\n",
        "The strong multicollinearity suggests some variables may be redundant for modeling. For instance, if Open and Close are tightly correlated, including both without adjustment could lead to model overfitting.\n",
        "\n",
        "Investors and analysts can use the strong correlation between High, Low, and Close to estimate price bands for risk management and stop-loss planning.\n",
        "\n",
        "Modelers can use insights from distribution skew to apply appropriate transformations or robust models that aren’t misled by outliers.\n",
        "\n"
      ],
      "metadata": {
        "id": "WmPK10bJgv9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null hypothesis - There is a no significant difference between High and Low stock prices.\n",
        "\n",
        "Alternate hypothesis - There is significant difference between High and Low stock prices."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from scipy.stats import ttest_ind\n",
        "t_statistic, p_value = ttest_ind(df['High'], df['Low'])\n",
        "print(\"T-Statistic:\", t_statistic)\n",
        "print(\"P-Value:\", p_value)\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. There is  significant difference between the mean of High and Low prices..\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. There is no significant difference between High and Low stock prices.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have done ttest_ind,  t individual group test between high and low stock prices. So, I could see T-Statistic of 2.05 and  P_value of 0.040. It tells us that we reject null hypothesis as it displays less than 0.05.\n"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have choosen this statistic test because here I could find the two groups actually had the same mean, what’s the probability that I would observe a difference this large or larger just due to random chance?"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis:  The true mean of Yes Bank’s monthly closing stock price is equal to ₹150.\n",
        "\n",
        "Alternate Hypothesis (H₁):  The true mean of the closing stock price is not equal to ₹150.\n"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy import stats\n",
        "sample_mean = df['Close'].mean()\n",
        "hypothesized_mean = 150\n",
        "\n",
        "t_statistic, p_value = stats.ttest_1samp(a=df['Close'], popmean=hypothesized_mean)\n",
        "\n",
        "print(\"T-statistic:\", t_statistic)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. The true mean of the closing stock price is not equal to ₹150.\")\n",
        "else:\n",
        "  print(\"Fail to reject the null hypothesis. The true mean of the closing stock price is equal to ₹150.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the One-Sample T-Test using stats.ttest_1samp"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are comparing the sample mean of a single numerical variable (Close prices) to a known or hypothesized population mean (₹150).\n",
        "\n",
        "The test determines if the sample provides enough statistical evidence to say that the true mean is significantly different from ₹150.\n",
        "\n",
        "The population standard deviation is unknown, so a Z-test is not appropriate — hence the t-test."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀):\n",
        "There is no linear correlation between the Open and Close stock prices of Yes Bank.\n",
        "\n",
        "Alternate Hypothesis (H₁):\n",
        "There is a linear correlation between the Open and Close stock prices."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy import stats\n",
        "\n",
        "correlation_coefficient, p_value = stats.pearsonr(df['Open'], df['Close'])\n",
        "\n",
        "print(\"Pearson correlation coefficient:\", correlation_coefficient)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. There is a linear correlation between the Open and Close stock prices.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. There is no linear correlation between the Open and Close stock prices.\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a strong, positive linear correlation between Open and Close prices, and it is statistically significant (since p < 0.05).\n",
        "This implies the opening price is a strong predictor of the closing price."
      ],
      "metadata": {
        "id": "7OHUvVvTsxvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I performed the Pearson’s Correlation Coefficient Test, using:\n",
        "stats.pearsonr(df['Open'], df['Close'])\n",
        "\n",
        "This test outputs:\n",
        "\n",
        "The Pearson correlation coefficient (r), which measures the strength and direction of linear association\n",
        "\n",
        "The p-value, which tells us whether the observed correlation is statistically significant\n"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose Pearson’s correlation test because:\n",
        "\n",
        "Both Open and Close are continuous, numerical variables\n",
        "\n",
        "We're interested in testing the strength and direction of their linear relationship\n",
        "\n",
        "Pearson’s r is the standard method for testing linear correlation when:\n",
        "\n",
        "The data is approximately normally distributed\n",
        "\n",
        "The relationship is assumed to be linear\n",
        "\n",
        "There are no strong outliers that could skew results\n",
        "\n",
        "This test helps assess whether one price (Open) can be linearly associated with another (Close) — useful in building predictive models or trading strategies."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no missing values ​​in the data set\n",
        "\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# visualize Outliers\n",
        "plt.figure(figsize=(12, 6))\n",
        "df[['Open', 'High', 'Low', 'Close']].boxplot()\n",
        "plt.xlabel('Price Type')\n",
        "plt.ylabel('Price')\n",
        "plt.title('Box Plot of Stock Prices')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This outlier is significant for this data set"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not needed for this dataset so skipping this step."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not needed for this dataset so skipping this step."
      ],
      "metadata": {
        "id": "ww0fHCoTumTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new\n",
        "\n",
        "df['Price_Range'] = df['High'] - df['Low']\n",
        "df['Price_Range']"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Price_Change'] = df['Close'] - df['Open']\n",
        "df['Price_Change']"
      ],
      "metadata": {
        "id": "_2N1I9KK7aH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Price_Change_Pct'] = ((df['Close'] - df['Open']) / df['Open']) * 100\n",
        "df['Price_Change_Pct']"
      ],
      "metadata": {
        "id": "AfEQz3U-7u_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Avg_Price'] = (df['Open'] + df['High'] + df['Low'] + df['Close']) / 4\n",
        "df['Avg_Price']"
      ],
      "metadata": {
        "id": "9t49L1-N79P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Volatility_Ratio'] = (df['High'] - df['Low']) / df['Open']\n",
        "df['Volatility_Ratio']"
      ],
      "metadata": {
        "id": "DrifZ7VL8D1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Is_Green_Month'] = (df['Close'] > df['Open']).astype(int)\n",
        "df['Is_Green_Month']"
      ],
      "metadata": {
        "id": "cwdr0MAS8OO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I would like to do type conversion of date in date format and sort it out for our future temporal work or calculation."
      ],
      "metadata": {
        "id": "laaoLUn585Mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the index to datetime format\n",
        "df.index = pd.to_datetime(df.index, format='%b-%y')\n",
        "\n",
        "df = df.sort_index()\n",
        "\n"
      ],
      "metadata": {
        "id": "OEMt1UKX9LQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets create rolling metric features."
      ],
      "metadata": {
        "id": "wmamfdvm9_FF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Close_Lag1'] = df['Close'].shift(1)\n",
        "df['Close_Rolling_Mean_3'] = df['Close'].rolling(window=3).mean()\n",
        "df['Close_Rolling_Std_3'] = df['Close'].rolling(window=3).std()"
      ],
      "metadata": {
        "id": "VFdp9MGO8ODO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "wUV0ddaT8nyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Select your features wisely to avoid overfitting\n",
        "#  lineplot for Constant Feature vs Target\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.lineplot(data=df, x='Open', y='Close')\n",
        "plt.title(\"open vs close\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "def calculate_vif(Dataframe):\n",
        "    # Handle missing values (NaN) by dropping rows containing them\n",
        "    Dataframe = Dataframe.dropna()\n",
        "\n",
        "    # Handle infinite values (inf) by replacing them with a large finite number\n",
        "    Dataframe = Dataframe.replace([np.inf, -np.inf], 1e9)\n",
        "\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"feature\"] = Dataframe.columns\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(Dataframe.values, i) for i in range(len(Dataframe.columns))]\n",
        "    return vif_data\n",
        "\n",
        "vif_sample_df = calculate_vif(df.copy()) # Create a copy of df to avoid modifying the original\n",
        "\n",
        "vif_sample_df\n"
      ],
      "metadata": {
        "id": "CPEosNQ05ToC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only numerical columns for correlation\n",
        "numerical_df = df.select_dtypes(include=np.number)\n",
        "\n",
        "# Handle infinite values by replacing them with a large finite number\n",
        "numerical_df = numerical_df.replace([np.inf, -np.inf], 1e9)\n",
        "\n",
        "# Drop rows with NaN values that might result from previous operations (like rolling calculations)\n",
        "numerical_df = numerical_df.dropna()\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = numerical_df.corr()\n",
        "\n",
        "# Plot the correlation heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap of Numerical Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2ifienPD6exx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VIF analysis revealed extreme multicollinearity among raw price features (Open, High, Low, Close) and derived metrics (Avg_Price, Price_Range, etc.), leading to VIF scores of inf. To mitigate this, we dropped redundant features and retained only independent, high-information features like Close_Lag1, Price_Change_Pct, and Volatility_Ratio. This step ensures model stability, especially for linear models."
      ],
      "metadata": {
        "id": "fsXRKfn_AWIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop redundant and multicollinear features\n",
        "features_to_drop = [\n",
        "    'Open', 'High', 'Low',\n",
        "    'Price_Range', 'Price_Change', 'Avg_Price',\n",
        "    'Close_Rolling_Mean_3'\n",
        "]\n",
        "\n",
        "df_reduced = df.drop(columns=features_to_drop)\n",
        "\n",
        "# Verify final columns\n",
        "print(df_reduced.columns)\n"
      ],
      "metadata": {
        "id": "_jPcHlX8BI6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have dropped redundant features step by step using a strategic approach to reduce multicollinearity, without losing important signals for prediction."
      ],
      "metadata": {
        "id": "ILTyiZVZBklT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used VIF to identify features that are highly correlated with each other.\n",
        "\n",
        "This is important because high multicollinearity can cause instability in regression-based models and lead to unreliable coefficient estimates.\n",
        "\n",
        "Features with very high VIF (approaching or at inf) were considered redundant and removed.\n",
        "\n",
        "Domain Knowledge & Redundancy Reduction\n",
        "\n",
        "I grouped raw features like Open, High, Low, Close and derived features such as Price_Change, Avg_Price, Price_Range.\n",
        "\n",
        "Since many derived features were based on the same components, I kept only one representative from each group to avoid duplication of information.\n",
        "\n",
        "Statistical Insight (T-tests and Correlation)\n",
        "\n",
        "I conducted T-tests and Pearson correlation to understand which features show a statistically significant relationship with the target (Close price).\n",
        "\n",
        "Features like Close_Lag1, Price_Change_Pct, and Volatility_Ratio showed meaningful signals."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Price_Change_Pct,  Volatility_Ratio\t, Close_Lag1\t, Close_Rolling_Std_3, Is_Green_Month\n",
        "\n",
        "These features were chosen because they:\n",
        "\n",
        "Are statistically relevant\n",
        "\n",
        "Are non-redundant (low multicollinearity)\n",
        "\n",
        "Add predictive value without overfitting\n",
        "\n",
        "Have interpretability for stakeholders"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
        "\n",
        "#  Start from df_reduced (already cleaned and with low VIF)\n",
        "df_reduced = df_reduced.copy()"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_reduced['MA10'] = df_reduced['MA10'].fillna(method='bfill')\n",
        "df_reduced['MA20'] = df_reduced['MA20'].fillna(method='bfill')\n",
        "df_reduced['Close_Lag1'] = df_reduced['Close_Lag1'].fillna(method='bfill')\n",
        "df_reduced['Close_Rolling_Std_3'] = df_reduced['Close_Rolling_Std_3'].fillna(method='bfill')"
      ],
      "metadata": {
        "id": "cDwRMfSPT1Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "features_to_scale = [\n",
        "    'Price_Change_Pct', 'Volatility_Ratio', 'Close_Lag1',\n",
        "    'Close_Rolling_Std_3', 'MA10', 'MA20'\n",
        "]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_scaled = df_reduced.copy()\n",
        "df_scaled[features_to_scale] = scaler.fit_transform(df_scaled[features_to_scale])\n"
      ],
      "metadata": {
        "id": "SSdPfwPkUi5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Doing log transformation for skewed features.\n",
        "\n",
        "df_log = df_scaled.copy()\n",
        "# Adding 1 to avoid log(0) issues\n",
        "df_log['Volatility_Ratio_log'] = np.log1p(df_log['Volatility_Ratio'])\n"
      ],
      "metadata": {
        "id": "HLBsMnBdUoqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To further normalize skewed variables.\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "pt = PowerTransformer()\n",
        "df_log[['Price_Change_Pct', 'Volatility_Ratio']] = pt.fit_transform(\n",
        "    df_log[['Price_Change_Pct', 'Volatility_Ratio']]\n",
        ")"
      ],
      "metadata": {
        "id": "zQJAGWUhUw0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#: Stationarity Transformation (for doing Time-Series) -- For models  ARIMA / SARIMA.\n",
        "df_stationary = df_reduced[['Close']].diff().dropna()\n"
      ],
      "metadata": {
        "id": "GqgzZFEcU_ZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To avoid redundancy and multicollinearity, dropping raw features.\n",
        "\n",
        "df_final = df_log.drop(['Volatility_Ratio'], axis=1)\n"
      ],
      "metadata": {
        "id": "2Tf_kjwhVMiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.head()"
      ],
      "metadata": {
        "id": "eJ2Vm0RYVSkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used StandardScaler from sklearn.preprocessing to scale the numerical features.\n",
        "\n",
        "StandardScaler transforms features so that they have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "This ensures that all features are on the same scale, which is important for algorithms that are distance-based (e.g., KNN, SVM) or sensitive to feature magnitude (e.g., Logistic Regression, Linear Regression).\n",
        "\n",
        "It also helps improve convergence speed in gradient-based models and ensures that no single feature dominates the model purely due to scale differences."
      ],
      "metadata": {
        "id": "FSGthssJV94-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "StandardScaler was the best fit because it maintains the shape of the feature distribution while normalizing the scale, making it suitable for most machine learning algorithms we plan to use."
      ],
      "metadata": {
        "id": "LJ98pswwWf23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not really necessary right now and because number of Features is Small and we  already Removed Redundant Feature and we dont have the risk of losing model Interpretability."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1️⃣ Separate features and target\n",
        "X = df_final.drop('Close', axis=1)  # All independent variables\n",
        "y = df_final['Close']               # Dependent variable\n",
        "\n",
        "# 2️⃣ Train-test split (80% train, 20% test for example)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used an 80:20 train-test split ratio.\n",
        "\n",
        "80% for training: This ensures the model has sufficient data to learn patterns, relationships, and variance in the features, reducing underfitting risk.\n",
        "\n",
        "20% for testing: This portion is kept completely unseen during training to evaluate the model’s generalization ability on new data.\n",
        "\n",
        "The 80:20 split is a widely accepted balance in machine learning because:\n",
        "\n",
        "It provides enough data for both learning and reliable evaluation.\n",
        "\n",
        "It works well for datasets of moderate size — large enough to learn from but not so small that evaluation metrics become unstable.\n",
        "\n",
        "It prevents data leakage by ensuring the testing set represents real-world unseen scenarios.\n",
        "\n",
        "If the dataset were extremely small, I would have considered k-fold cross-validation instead to maximize training data while still validating performance."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our Yes Bank stock prices dataset, handling an imbalanced dataset is generally not required because we are mainly focussing on regression and time series concepts rather than classification problem."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "X = df_final.drop('Close', axis=1)\n",
        "y = df_final['Close']\n",
        "\n",
        "# Fit the Algorithm\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred = lr_model.predict(X_test)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Calculate the evaluation metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model: Linear Regression\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R² Score: {r2:.2f}\")"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "metrics = ['RMSE', 'R² Score']\n",
        "scores = [rmse, r2]  # from previous code\n",
        "\n",
        "plt.bar(metrics, scores, color=['skyblue', 'lightgreen'])\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model 1: Linear Regression Performance')\n",
        "for i, score in enumerate(scores):\n",
        "    plt.text(i, score + 0.01, f\"{score:.2f}\", ha='center')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QdNgvSYIiF1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression is a supervised learning algorithm that models the relationship between a dependent variable (Close price in our case) and one or more independent variables (our other stock features) by fitting a straight line (or hyperplane in multiple dimensions) that minimizes the error between predicted and actual values.\n",
        "The goal is to learn weights for each feature that best predict the target."
      ],
      "metadata": {
        "id": "6-4jOJRqih8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Define model\n",
        "ridge = Ridge()\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'alpha': [0.01, 0.1, 1, 10, 100]\n",
        "}\n",
        "\n",
        "# GridSearch with Cross-Validation (5-fold)\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=ridge,\n",
        "    param_grid=param_grid,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit on training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_ridge = grid_search.best_estimator_\n",
        "\n",
        "# Predict\n",
        "y_pred = best_ridge.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"R² Score:\", r2)\n",
        "\n",
        "# Cross-validation score\n",
        "cv_scores = cross_val_score(best_ridge, X_train, y_train, cv=5, scoring='r2')\n",
        "print(\"Cross-Validation R² Scores:\", cv_scores)\n",
        "print(\"Mean CV R² Score:\", cv_scores.mean())\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV for hyperparameter tuning.\n",
        "\n",
        "Reason for Choosing GridSearchCV:\n",
        "\n",
        "Exhaustive Search → GridSearchCV systematically tries all possible combinations of the given hyperparameters, ensuring that the optimal set is not missed.\n",
        "\n",
        "Cross-Validation Integrated → It evaluates each hyperparameter set using k-fold cross-validation, which reduces overfitting risk and gives a better estimate of performance.\n",
        "\n",
        "Deterministic Output → Given the same data and parameter grid, GridSearchCV produces consistent, reproducible results, which is important for reliable model comparison.\n",
        "\n",
        "Best for Small Search Spaces → Since my parameter space (e.g., alpha in Ridge Regression) was relatively small, GridSearchCV was computationally feasible and guaranteed to find the best configuration.\n",
        "\n"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = {\n",
        "    'RMSE': 13.280393012884526,\n",
        "    'R² Score': 0.9804876544056066,\n",
        "    'Mean CV R² Score': 0.9649074401795883\n",
        "}\n",
        "\n",
        "# Create bar chart\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(metrics.keys(), metrics.values(), color=['skyblue', 'lightgreen', 'salmon'])\n",
        "plt.ylabel('Score / Value')\n",
        "plt.title('Evaluation Metrics - Tuned Linear Regression Model')\n",
        "\n",
        "# Annotate bars\n",
        "for i, (metric, value) in enumerate(metrics.items()):\n",
        "    plt.text(i, value + 0.005, f'{value:.3f}', ha='center', fontsize=10)\n",
        "\n",
        "plt.ylim(0, 1.1 * max(metrics.values()))  # Adjust Y limit\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8kkvZUI0wcIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying hyperparameter tuning using GridSearchCV, the model’s performance remained almost the same in terms of RMSE and R² on the test set. However, the cross-validation results show that the tuned model generalizes well across different folds."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Model 2: Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Model: Random Forest Regressor\")\n",
        "print(f\"RMSE: {rmse_rf:.2f}\")\n",
        "print(f\"R² Score: {r2_rf:.2f}\")\n"
      ],
      "metadata": {
        "id": "RfoKkfODyW3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Metrics for Model 2\n",
        "metrics = ['RMSE', 'R² Score']\n",
        "values = [15.71, 0.97]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "bars = plt.bar(metrics, values, color=['skyblue', 'orange'])\n",
        "\n",
        "# Add values on top of bars\n",
        "for bar, value in zip(bars, values):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "             f\"{value:.2f}\", ha='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.title('Evaluation Metric Score Chart - Model 2', fontsize=14)\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, max(values) + 5)  # Extra space above bars\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second model is a Random Forest Regressor, an ensemble learning method that constructs multiple decision trees during training and outputs the mean prediction of the individual trees. It is particularly effective at capturing complex, non-linear relationships between features and the target variable.\n",
        "\n",
        "Random Forest reduces overfitting through averaging and can handle both numerical and categorical features without requiring strict scaling assumptions."
      ],
      "metadata": {
        "id": "enAH48QJxgnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got RMSE score of 15.71 and R2 score of 0.97 from our random forest model."
      ],
      "metadata": {
        "id": "CMVR4HqOxqw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define base model\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "# GridSearchCV for tuning\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,                  # 5-fold cross-validation\n",
        "    scoring='r2',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit model on training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "# Evaluation metrics\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R² Score: {r2:.2f}\")\n",
        "\n",
        "# Cross-validation scores\n",
        "cv_scores = cross_val_score(best_rf, X_train, y_train, cv=5, scoring='r2')\n",
        "print(\"Cross-Validation R² Scores:\", cv_scores)\n",
        "print(\"Mean CV R² Score:\", np.mean(cv_scores))"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "cwu2zTBG0LyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used GridSearchCV for hyperparameter optimization.\n",
        "\n",
        "Reason:\n",
        "GridSearchCV exhaustively searches through a manually specified subset of hyperparameters, testing all possible combinations within the defined grid. It performs k-fold cross-validation (here, 5-fold) for each combination, ensuring the selected parameters generalize well to unseen data.\n",
        "\n",
        "This method was chosen because:\n",
        "\n",
        "The parameter space for Random Forest was small enough to allow exhaustive search without excessive computational cost.\n",
        "\n",
        "It ensures we do not miss the best-performing parameter set within the given grid.\n",
        "\n",
        "It directly integrates cross-validation, which provides a reliable estimate of model performance for each hyperparameter configuration."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics before and after tuning\n",
        "metrics = ['RMSE', 'R² Score']\n",
        "before_tuning = [15.71, 0.97]\n",
        "after_tuning = [16.16, 0.97]\n",
        "\n",
        "x = range(len(metrics))\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(x, before_tuning, width=0.4, label='Before Tuning', align='center')\n",
        "plt.bar([i + 0.4 for i in x], after_tuning, width=0.4, label='After Tuning', align='center')\n",
        "\n",
        "plt.xticks([i + 0.2 for i in x], metrics)\n",
        "plt.ylabel('Score')\n",
        "plt.title('Evaluation Metric Score Chart - Model 2 (Tuned)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x4K9OZym1SkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After tuning the Random Forest Regressor with GridSearchCV, the performance improved slightly in terms of cross-validation consistency, though the RMSE increased marginally. The tuned model shows more stable R² scores across folds, indicating better generalization.\n",
        "\n",
        "RMSE\t - before 15.71  after - 16.16\n",
        "\n",
        "R2     - before 0.97\t after - 0.97\n",
        "\n",
        "Mean CV R² Score - 0.9700\n",
        "\n",
        "CV R² Scores (per fold)\t - [0.9075, 0.9884, 0.9856, 0.9816, 0.9871]:"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. RMSE (Root Mean Squared Error)\n",
        "What it means:\n",
        "RMSE measures the average prediction error of the model in the same units as the target variable. Lower RMSE means more accurate predictions.\n",
        "\n",
        "In our context:\n",
        "Since our target is stock price (or closing price), an RMSE of ~15 means the model’s predictions are, on average, off by about ₹15 from the actual price.\n",
        "\n",
        "Business impact:\n",
        "\n",
        "Lower RMSE → better price prediction accuracy → more reliable for decision-making (e.g., timing buy/sell).\n",
        "\n",
        "High RMSE could lead to poor entry/exit decisions, affecting profitability.\n",
        "\n",
        "2. R² Score (Coefficient of Determination)\n",
        "What it means:\n",
        "R² shows the proportion of variance in the target variable explained by the model (0 to 1 range). Higher R² means the model fits the data better.\n",
        "\n",
        "In our context:\n",
        "An R² of ~0.97 means 97% of the variation in stock prices is explained by our model’s features.\n",
        "\n",
        "Business impact:\n",
        "\n",
        "High R² means the model captures most market trends and patterns.\n",
        "\n",
        "This improves trust in predictions for business planning, risk management, and investment strategies.\n",
        "\n",
        "3. Cross-Validation R² Score (Mean CV R²)\n",
        "What it means:\n",
        "Shows how well the model generalizes to unseen data by training and testing on multiple subsets.\n",
        "\n",
        "In our context:\n",
        "A mean CV R² of ~0.97 means the model is consistent and not overfitting to historical data.\n",
        "\n",
        "Business impact:\n",
        "\n",
        "Ensures reliability across different time periods and market conditions.\n",
        "\n",
        "Reduces risk of making trading decisions based on over-optimistic models.\n",
        "\n",
        "Overall Business Impact\n",
        "High R² + Low RMSE → Model can accurately and consistently predict stock prices, aiding in:\n",
        "\n",
        "Better trading signals → optimized buy/sell points.\n",
        "\n",
        "Risk mitigation → avoiding large price surprises.\n",
        "\n",
        "Confidence in automated strategies → reduced manual intervention."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade xgboost\n"
      ],
      "metadata": {
        "id": "wtp4o356-VGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Define model (no early stopping to avoid compatibility issues)\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    eval_metric='rmse'  # Works in older versions\n",
        ")\n",
        "\n",
        "# Fit model normally\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R² Score: {r2:.2f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Replace with your actual values\n",
        "rmse = 13.21\n",
        "r2 = 0.98\n",
        "\n",
        "metrics = {'RMSE': rmse, 'R² Score': r2}\n",
        "plt.bar(metrics.keys(), metrics.values(), color=['orange', 'green'])\n",
        "plt.ylabel('Score')\n",
        "plt.title('XGBoost Model Evaluation Metrics')\n",
        "plt.ylim(0, max(metrics.values()) + 1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We implemented XGBoost Regressor, which is an optimized gradient boosting framework designed for speed and performance. It builds an ensemble of decision trees sequentially, where each new tree corrects errors made by the previous ones.\n",
        "XGBoost is well-suited for tabular datasets, handles missing values internally, and can capture complex non-linear relationships in the data."
      ],
      "metadata": {
        "id": "j-kMWupV_YpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Define the model\n",
        "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Parameter grid for tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7, None],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'subsample': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_grid=param_grid,\n",
        "    scoring='r2',\n",
        "    cv=5,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predict\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"R² Score:\", r2)\n",
        "\n",
        "# Cross-validation scores\n",
        "cv_scores = cross_val_score(grid_search.best_estimator_, X_train, y_train, cv=5, scoring='r2')\n",
        "print\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used GridSearchCV for hyperparameter optimization.\n",
        "\n",
        "Reason: GridSearchCV exhaustively searches through a manually specified subset of hyperparameters, testing all possible combinations within the defined grid. It performs k-fold cross-validation (here, 5-fold) for each combination, ensuring the selected parameters generalize well to unseen data."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No prominent improvement seen as rmse went from  13.21 to 13.14 and r2 score remains the same 0.98"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your actual values\n",
        "rmse = 13.14\n",
        "r2 = 0.98\n",
        "\n",
        "metrics = {'RMSE': rmse, 'R² Score': r2}\n",
        "plt.bar(metrics.keys(), metrics.values(), color=['orange', 'green'])\n",
        "plt.ylabel('Score')\n",
        "plt.title('XGBoost Model Evaluation Metrics')\n",
        "plt.ylim(0, max(metrics.values()) + 1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IV4c-YdFH4Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose RMSE it penalizes larger errors more than MAE because of the squaring before averaging. This is important because large prediction errors in stock prices can lead to disproportionately large financial losses.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Reducing large errors is critical to avoid catastrophic investment decisions, making RMSE valuable for risk-sensitive scenarios.\n",
        "\n",
        "And I chose R2 it measures the proportion of variance in the actual stock prices that the model explains through its predictions.\n",
        "\n",
        "Ranges from 0 to 1 (or can be negative if the model performs worse than a simple mean prediction).\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "R² is useful as a general model quality check but should not be the sole metric for business decisions.\n",
        "\n",
        "It should be used in combination with MAE, RMSE, directional accuracy, and profit simulation metrics for a fuller picture."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose tuned XGBRegressor as my final prediction model as a succesful one by comparing others. I got 13.14 as a rmse score and 0.98 as R2 score which is comparatively good than the other models which we have implemented."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shap"
      ],
      "metadata": {
        "id": "4HuAdh_hK_4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "\n",
        "target = 'Close'\n",
        "\n",
        "# Prepare features and target\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "explainer = shap.TreeExplainer(model)\n",
        "\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "shap.summary_plot(shap_values, X_test)\n"
      ],
      "metadata": {
        "id": "NXvI0uNALHvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features like Low, Avg_Price, and High have the most spread in SHAP values, meaning they influence the model's prediction strongly with close feature.\n",
        "\n",
        "Similarly, Avg_Price and High also show a trend where higher values push the prediction higher.\n",
        "\n",
        "Features like Price_Change_Pct, MA10, and Volatility_Ratio have little impact since their SHAP values cluster around zero."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion, we can implement our tuned XGBRegressor to make the prediction of the stock prices as it give the best result comparatively as we got 13.14 as a rmse score and 0.98 as R2 score  which displays less error between the predicted and actual values."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}